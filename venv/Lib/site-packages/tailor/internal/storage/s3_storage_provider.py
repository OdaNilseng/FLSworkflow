# -*- coding: utf-8 -*-

import fnmatch
import json
import uuid
from os.path import basename
from pathlib import PurePath, Path
from posixpath import join as posixjoin
from typing import Optional, List, Union, Sequence

import boto3
from boto3 import s3
from botocore.exceptions import ClientError
from tailor.internal.storage.storage_provider import StorageProvider
from tailor.internal.utils import get_logger


class S3StorageProvider(StorageProvider):
    def __init__(self, bucket_name: str):
        self.logger = get_logger('s3_storage')
        self.bucket_name: str = bucket_name
        self.resource = boto3.resource('s3')
        self.client: boto3.client = self.resource.meta.client
        self.bucket: s3.Bucket = self.resource.Bucket(self.bucket_name)
        self.owner_id = self.bucket.Acl().owner['ID']

    def __key_exists(self, key):
        exists = False
        try:
            self.resource.Object(self.bucket_name, key).load()
        except ClientError as e:
            if e.response['Error']['Code'] == "404":
                exists = False
            else:
                self.logger.error("Unknown error", exc_info=1)
                raise
        else:
            exists = True
        return exists

    def __prefix(self, storage_key):
        return storage_key + '/'

    def __non_user_prefix(self, storage_key):
        return self.__prefix(storage_key) + '_/'

    def __tags_prefix(self, storage_key):
        return self.__non_user_prefix(storage_key) + 'tags/'

    def __scope_prefixes_prefix(self, storage_key):
        return self.__non_user_prefix(storage_key) + 'scope_prefixes/'

    def __initialize_storage_key(self, storage_key):

        # create 'folder' on S3
        self.client.put_object(
            Bucket=self.bucket_name, Body='', Key=self.__prefix(storage_key))

        # create 'folder' for non-user files
        self.client.put_object(
            Bucket=self.bucket_name, Body='', Key=self.__non_user_prefix(storage_key))

        # create 'folder' for tags
        self.client.put_object(
            Bucket=self.bucket_name, Body='', Key=self.__tags_prefix(storage_key))

    def _add_tag(self, storage_key, tag, paths):
        self.client.put_object(
            Bucket=self.bucket_name,
            Body=json.dumps(paths).encode(),
            Key=self.__tags_prefix(storage_key) + tag)

    def _load_tags(self, storage_key):
        tags_prefix = self.__tags_prefix(storage_key)
        objs = self.client.list_objects(Bucket=self.bucket_name, Prefix=tags_prefix)
        obj_content = objs.get('Contents')
        if not obj_content:
            return {}
        tags = {}
        # do not use objdict['Key'] == tags_prefix
        for objdict in obj_content:
            if not objdict['Key'] == tags_prefix:
                tag = PurePath(objdict['Key']).name
                obj = self.resource.Object(self.bucket_name, objdict['Key'])
                tags[tag] = json.loads(obj.get()['Body'].read().decode('utf-8'))
        return tags

    def _add_scope_prefix(self, storage_key, scope_prefix):
        identifier = str(uuid.uuid1())
        self.client.put_object(
            Bucket=self.bucket_name,
            Body=json.dumps(scope_prefix).encode(),
            Key=self.__scope_prefixes_prefix(storage_key) + identifier)

    def _load_scope_prefixes(self, storage_key):
        scope_prefixes_prefix = self.__scope_prefixes_prefix(storage_key)
        objs = self.client.list_objects(
            Bucket=self.bucket_name, Prefix=scope_prefixes_prefix)
        obj_content = objs.get('Contents')
        if not obj_content:
            return []
        scope_prefixes = []
        # do not use objdict['Key'] == scope_prefixes_prefix
        for objdict in obj_content:
            if not objdict['Key'] == scope_prefixes_prefix:
                obj = self.resource.Object(self.bucket_name, objdict['Key'])
                scope_prefixes.append(
                    json.loads(obj.get()['Body'].read().decode('utf-8')))
        return scope_prefixes

    def new_storage_key(self) -> str:
        storage_key = str(uuid.uuid4())
        self.__initialize_storage_key(storage_key)
        return storage_key

    def get_tags(self, storage_key: str) -> dict:
        tags = self._load_tags(storage_key)
        for tag in tags:
            if tags[tag] == '*SCOPED*':
                tags[tag] = self.get_file_list(storage_key, name_filter=tag + '/*')
        return tags

    def get_detailed_file_list(self, storage_key: str, name_filter: str = '*') -> List[
        dict]:
        prefix = self.__prefix(storage_key)

        result = []
        for file in self.__get_all_s3_objects(prefix):
            key = file['Key']
            file_name: str = key[len(prefix):]
            if file_name and not file_name.startswith('_/') \
                    and fnmatch.fnmatch(file_name, name_filter):
                url = self.__create_presigned_url(key)
                result.append({'file_name': file_name, 'url': url})

        return result

    # Reads all pages of objects from s3.
    # Note we are using old-style pagination, i.e. not Pagination object,
    # in order to support testing with moto
    def __get_all_s3_objects(self, prefix):
        continuation_token = None
        while True:
            list_kwargs = dict(MaxKeys=1000)
            if continuation_token:
                list_kwargs['ContinuationToken'] = continuation_token
            response = self.client.list_objects_v2(Bucket=self.bucket_name,
                                                   Prefix=prefix)
            yield from response.get('Contents', [])
            if not response.get('IsTruncated'):  # At the end of the list
                break
            continuation_token = response.get('NextContinuationToken')

    # This is a local operation with the s3 client, i.e. it does not make a network call
    def __create_presigned_url(self, key):
        params = {
            'Bucket': self.bucket_name,
            'Key': key}
        one_day = 86400
        url = self.client.generate_presigned_url('get_object',
                                                 Params=params,
                                                 ExpiresIn=one_day)
        return url

    def upload(
            self,
            storage_key: str,
            filename: Union[str, Sequence[str]],
            path_prefix: str = '',
            tag: Optional[str] = None
    ) -> List[str]:

        if not isinstance(filename, str):
            # assume sequence
            fnames = []
            for fn in filename:
                fnames.append(self.upload(storage_key, fn, path_prefix, tag))
            if tag:
                self._add_tag(storage_key, tag, fnames)
            return fnames
        else:
            base_filename = basename(filename)
            if tag:
                key = posixjoin(storage_key, path_prefix, tag, base_filename)
            else:
                key = posixjoin(storage_key, path_prefix, base_filename)
            # Upload the file to S3 and give the target bucket's owner full access to it
            self.client.upload_file(filename, self.bucket_name, key,
                                    ExtraArgs={
                                        'GrantFullControl': f'id="{self.owner_id}"'
                                    })
            if tag:
                fname = posixjoin(path_prefix, tag, base_filename)
            else:
                fname = posixjoin(path_prefix, base_filename)
            if tag:
                self._add_tag(storage_key, tag, fname)
            return fname

    def upload_scoped(
            self,
            storage_key: str,
            filename: Union[str, Sequence[str]],
            tag: str,
            scope_indices: List[int],
            path_prefix: str = ''
    ) -> List[str]:

        scope_prefix = tag
        for i in scope_indices:
            scope_prefix += '/' + str(i)
        fnames = self.upload(storage_key, filename, scope_prefix)
        self._add_tag(storage_key, tag, '*SCOPED*')  # add/overwrite tag
        self._add_scope_prefix(storage_key, scope_prefix)
        return fnames

    def download(
            self,
            storage_key: str,
            fname_or_tag: Union[str, Sequence[str]],
            target_dir: str = '.',
            filename_prefix: str = ''
    ) -> List[str]:

        tags = self.get_tags(storage_key)
        if not isinstance(fname_or_tag, str):
            # fname_or_tag is sequence of filenames or tags
            fnames = []
            for fn in fname_or_tag:
                downloaded = self.download(storage_key, fn, target_dir)
                if isinstance(downloaded, list):
                    fnames.extend(downloaded)
                else:
                    fnames.append(downloaded)
            return fnames
        elif fname_or_tag in tags:
            if self._load_tags(storage_key)[fname_or_tag] == '*SCOPED*':
                return self.download_scoped(storage_key, fname_or_tag, [], target_dir)
            return self.download(storage_key, tags[fname_or_tag], target_dir)
        else:
            key = posixjoin(self.__prefix(storage_key), fname_or_tag)
            local_path = Path(target_dir)
            local_filename = local_path / (filename_prefix + basename(fname_or_tag))
            if self.__key_exists(key):
                local_filename.parent.mkdir(parents=True, exist_ok=True)
                self.bucket.download_file(key, local_filename.as_posix())
                return local_filename
            else:
                raise ValueError(
                    'File {}, not found under storage key {} '
                    ''.format(fname_or_tag, storage_key))

    def download_scoped(
            self,
            storage_key: str,
            fname_or_tag: str,
            scope_indices: List[int],
            target_dir: str = '.'
    ) -> List[str]:

        if fname_or_tag in self.get_file_list(storage_key):

            fname = fname_or_tag
            p = PurePath(fname)
            scope_prefix = p.parent.as_posix()
            scope_prefixes = self._load_scope_prefixes(storage_key)
            if scope_prefix == '.' or scope_prefix not in scope_prefixes:
                # file not uploaded from a duplicate, do standard download
                return self.download(storage_key, fname, target_dir)
            else:
                # scoped_target_dir = posixjoin(
                #     target_dir, self._get_target_dir(scope_prefix, scope_indices))
                # return self.download(storage_key, fname, scoped_target_dir)
                filename_prefix = self._get_filename_prefix(scope_prefix, scope_indices)
                return self.download(storage_key, fname, '.', filename_prefix)

        elif fname_or_tag in self.get_tags(storage_key):

            tag = fname_or_tag
            if not self._load_tags(storage_key)[tag] == '*SCOPED*':
                # tag not uploaded from duplicate
                # do standard download
                return self.download(storage_key, fname_or_tag, target_dir)

            fnames = self.get_scoped_file_list(storage_key, tag, scope_indices)

            downloaded_fnames = []
            for fname in fnames:
                downloaded_fnames.append(self.download_scoped(
                    storage_key, fname, scope_indices, target_dir))
            return downloaded_fnames
        else:
            raise ValueError(
                f'File or tag "{fname_or_tag}" not found in storage resource with key '
                f'{storage_key}'
            )

    def delete_all(self, storage_key: str) -> None:
        objs = self.client.list_objects(Bucket=self.bucket_name,
                                        Prefix=self.__prefix(storage_key))
        if 'Contents' in objs:
            objects = [{'Key': x['Key']} for x in objs['Contents']]
            response: dict = self.bucket.delete_objects(Delete={'Objects': objects})
            if "Errors" in response.keys():
                self.logger.error(f"Errors deleting objects: {response['Errors']}")
                return False

        return True
