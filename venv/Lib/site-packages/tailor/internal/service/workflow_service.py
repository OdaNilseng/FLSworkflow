# -*- coding: utf-8 -*-
from collections import defaultdict
from datetime import datetime
from typing import List, Optional

from tailor.api.definition import TaskDefinition, DAGTask
from tailor.exceptions import LockedDocumentError
from tailor.internal.domain.single_task import SingleTaskState
from tailor.internal.domain.workflow import WorkflowModel
from tailor.internal.domain.workflow import WorkflowRepository
from tailor.internal.domain.workflow.workflow_model import ExternalReferencesModel
from tailor.internal.service.context_service import ContextService
from tailor.internal.service.run_service import RunService
from tailor.internal.service.single_task_service import SingleTaskService
from tailor.internal.service.storage_service import StorageService
from tailor.internal.utils import format_traceback


class WorkflowService:
    def __init__(self, project_name: str = None):
        self.repository: WorkflowRepository = WorkflowRepository(project_name)
        self.single_task_service: SingleTaskService = SingleTaskService(project_name)
        self.run_service: RunService = RunService(project_name)
        self.context_service: ContextService = ContextService(project_name)
        self.storage_service: StorageService = StorageService(project_name)
        self.project_name = project_name

    def mark_run_completed(self, run_id,
                           state: SingleTaskState = SingleTaskState.COMPLETED) -> WorkflowModel:
        updated_run = self.run_service.update_state(run_id, state)

        return self.refresh(updated_run.task_id)

    def mark_run_failed(self, run_id, e: Exception,
                        error_message: str = None) -> WorkflowModel:
        traceback_str = ''.join(format_traceback(e))
        if not error_message and hasattr(e, 'message'):
            error_message: str = e.message
        updated_run = self.run_service.update_state(run_id, SingleTaskState.FAILED,
                                                    error_message=error_message,
                                                    traceback=traceback_str)

        return self.refresh(updated_run.task_id)

    def insert(self, workflow: WorkflowModel):
        return self.repository.insert(workflow)

    def insert_many(self, workflows: List[WorkflowModel]):
        return self.repository.insert_many(workflows)

    def replace(self, workflow: WorkflowModel, preserve_lock=True) -> WorkflowModel:
        wf_dict = self.repository.replace(workflow, preserve_lock)
        if not wf_dict:
            return None
        return WorkflowModel.from_dict(wf_dict)

    def delete_by_id_cascade(self, workflow_id) -> bool:
        # TODO should be an atomic operation in mongodb
        workflow = self.find_by_id(workflow_id)
        if not workflow:
            return False

        context = self.context_service.find_by_id(workflow.context_id)
        storage_key = context.storage_key
        if not storage_key:
            return False

        tasks = self.single_task_service.find_by_workflow_id(workflow_id)
        for t in tasks:
            self.run_service.delete_by_task_id(t.id)

        self.single_task_service.delete_by_workflow_id(workflow_id)
        self.context_service.delete_by_id(context.id)
        self.repository.delete_by_id(workflow_id)

        return self.storage_service.delete_all(storage_key)

    def find_by_user(self, user_uuid: str, limit: int) -> WorkflowModel:
        wf_dicts = self.repository.find_by_user(user_uuid, limit)
        if not wf_dicts:
            return None
        return [WorkflowModel.from_dict(wf) for wf in wf_dicts]

    def find_by_id(self, workflow_id, project_filter=None) -> WorkflowModel:
        wf_dict = self.repository.find_by_id(workflow_id)
        if not wf_dict:
            return None
        return WorkflowModel.from_dict(wf_dict)

    def find_by_task_id(self, task_id) -> WorkflowModel:
        task = self.single_task_service.find_by_id(task_id)
        if not task:
            return None
        return self.find_by_id(task.wf_id)

    def find_all(self, query: Optional[str], limit: int, project_filter=None) -> List[
        WorkflowModel]:
        wf_dicts = self.repository.find_all(query, limit)
        return [WorkflowModel.from_dict(wf) for wf in wf_dicts]

    def find_workflow_names(self, query: Optional[str] = None) -> List[str]:
        return self.repository.find_all_names(query)

    def update_task_states(self, wf_id, task_states) -> WorkflowModel:
        try:
            with self.repository.lock_manager(wf_id):
                wf = self.find_by_id(wf_id)
                wf.task_states.update(task_states)
                return self.replace(wf, preserve_lock=True)
        except LockedDocumentError:
            # do something
            raise

    def as_parent_links(self, links):
        # child: parents links
        # this does not include root tasks...
        c_p_links = defaultdict(list)
        for p, c in links.items():
            for ci in c:
                c_p_links[ci].append(p)
        return dict(c_p_links)

    def create(self, wf_def: TaskDefinition,
               external_references: ExternalReferencesModel,
               name: Optional[str] = None,
               inputs: dict = None,
               storage_key: Optional[str] = None,
               worker: Optional[str] = None
               ) -> WorkflowModel:
        """
        Add a workflow definition to the database for execution.

        Parameters
        ----------
        wf_def : TaskDefinition
            Workflow definition.
        name : str, optional
            Provide a name for this workflow run.
        inputs : dict
            input data which can be queried from tasks during workflow execution.
            the data must be JSON/BSON serializable.
        storage_key : BaseFileStore, optional
            Provide a storage object. The storage object must include all files that
            are referenced (through file tags) in the workflow definition. This
            storage object becomes the associated storage object for this particular
            workflow run.
        worker : str, optional
            Specify a worker name for which the workflow shall be executed.
        external_references: ExternalReferencesModel
            Specify the metadata for the workflow

        Returns
        -------
        wf_id : int
            The unique id for this workflow run.

        """

        # TODO: aux_storage: add additional storage resources where the workflow
        #       can look for files

        if not isinstance(wf_def, DAGTask):
            wf_def = DAGTask(wf_def)

        # get Tasks objects and links
        task_defs, p_c_links, c_p_links = wf_def.get_task_defs_and_links()

        wf_id = self.repository._get_new_id(id_type='workflow')

        # create Tasks and id-based links
        tasks, p_c_links = self.single_task_service.create_tasks_and_links(wf_id,
                                                                           task_defs,
                                                                           p_c_links,
                                                                           worker)

        # create a storage key if not provided
        if storage_key is None:
            storage_key = self.storage_service.new_storage_key()

        # create context
        context = self.context_service.create(inputs, {}, storage_key)

        task_states = {t.id: t.state for t in tasks}
        root_tasks_ids = self.determine_root_ids(p_c_links)
        for task_id in root_tasks_ids:
            task_states[task_id] = SingleTaskState.READY
        for task in tasks:
            if task.id in root_tasks_ids:
                task.state = SingleTaskState.READY
        self.single_task_service.insert_many(tasks)

        wf = WorkflowModel(
            wf_id=wf_id,
            name=name or 'Unnamed workflow',
            wf_def=wf_def.to_dict(),
            links=p_c_links,
            context_id=context.id,
            task_states=task_states,
            worker=worker,
            external_references=external_references
        )

        self.repository.insert(wf)

        return wf

    def determine_root_ids(self, links: dict):
        all_ids = set(links)
        child_ids = set(self.as_parent_links(links).keys())
        root_ids = all_ids.difference(child_ids)
        return list(root_ids)

    def pretty_print(self, wf):
        lines = []
        # columns
        tf = '{:^6.6}'  # task id
        n1 = '{:<21.20}'  # name
        p1 = '{:^22.21}'  # parents
        n2 = '{:<19.19}..'  # name
        p2 = '{:^20.20}..'  # parents
        typ = '{:^12.12}'  # type
        s = '{:^12.12}'  # state

        row = '|' + tf + '|' + n1 + '|' + p1 + '|' + typ + '|' + s + '|\n'
        top = '+' + '-' * 77 + '+' + '\n'
        vsep = '+' + '-' * 6 + '+' + '-' * 21 + '+' + '-' * 22 + '+' + '-' * 12 + '+' + '-' * 12 + '+\n'
        header = f'| Workflow {wf.id}: {wf.name}'
        header = header + ' ' * (78 - len(header)) + '|\n'
        colheader = row.format('id', ' Task name', 'Parents', 'Type', 'State')
        lines.append(top)
        lines.append(header)
        lines.append(vsep)
        lines.append(colheader)
        lines.append(vsep)

        added_tasks = set()
        rows_dict = {}

        def add_row(tid):
            if not tid in added_tasks:
                added_tasks.add(tid)
                t = self.single_task_service.find_by_id(tid)
                task_name = ' ' + t.task_def['name']
                n = n1 if len(task_name) < 21 else n2
                parents = str(wf.parent_links[tid])[
                          1:-1] if tid in wf.parent_links else '-'
                p = p1 if len(parents) < 22 else p2
                row = '|' + tf + '|' + n + '|' + p + '|' + typ + '|' + s + '|\n'
                rows_dict[tid] = row.format(
                    str(tid),
                    task_name,
                    parents,
                    t.task_def['type'].upper(),
                    t.state.name
                )

                for cid in wf.links[tid]:
                    add_row(cid)

        for rtid in wf.root_task_ids:
            add_row(rtid)

        lines.extend([v for k, v in sorted(rows_dict.items())])
        lines.append(vsep)

        return ''.join(lines)

    def update_links(self, wf_id, links) -> WorkflowModel:
        try:
            with self.repository.lock_manager(wf_id):
                wf = self.find_by_id(wf_id)
                for pid, cids in links.items():
                    if pid in wf.links:
                        wf.links[pid].extend(cids)
                    else:
                        wf.links[pid] = cids
                return self.replace(wf, preserve_lock=True)
        except LockedDocumentError:
            # do something
            raise

    def refresh_wf(self, wf: WorkflowModel, task_id, updated_ids=None):

        # these are the task ids to re-enter into the database
        updated_ids = updated_ids if updated_ids else set()

        task = self.single_task_service.find_by_id(task_id)
        prev_state = task.state

        # skip if task is stopped
        if task.state == SingleTaskState.STOPPED:
            wf.task_states[task_id] = task.state
            return updated_ids

        completed_parent_states = [SingleTaskState.COMPLETED]
        # TODO: implement allow_failed_parents:
        #  (must be specified in taskdef or workflow def)
        # if task.allow_failed_parents:
        #     completed_parent_states.append(SingleTaskState.FAILED)

        # check parent states for any that are not completed
        for parent in wf.parent_links.get(task_id, []):
            if wf.task_states[parent] not in completed_parent_states:
                state = SingleTaskState.WAITING
                break

        else:  # not STOPPED, and all parents are done running.
            # Now the state depends on the launch status
            # State depends on run whose state has the highest rank
            run = self.run_service.get_representative_run(task)
            state = run.state if run else SingleTaskState.READY

        task.state = state

        # Store the updated task
        self.single_task_service.replace(task)

        # bring wf.task_states in sync with task_states in db
        wf.task_states[task_id] = state

        if state != prev_state:
            updated_ids.add(task_id)

            # refresh all the children that could possibly now be READY to run
            # TODO: if state in [SingleTaskState.COMPLETED, SingleTaskState.FAILED]:  # for allow_failed_parents children
            if state == SingleTaskState.COMPLETED:
                for child_id in wf.links[task_id]:
                    updated_ids = updated_ids.union(
                        self.refresh_wf(wf, child_id, updated_ids))

        wf.updated_on = datetime.utcnow()

        return updated_ids

    def refresh(self, task_id) -> WorkflowModel:
        """
        Refresh the workflow containing task with *task_id*
        """
        wf_id = self.single_task_service.get_workflow_id(task_id)
        try:
            with self.repository.lock_manager(wf_id):
                workflow: WorkflowModel = self.find_by_id(wf_id)
                self.refresh_wf(workflow, task_id)
                self.replace(workflow)
        except LockedDocumentError:
            # do something
            raise
        return self.find_by_id(workflow.id)
