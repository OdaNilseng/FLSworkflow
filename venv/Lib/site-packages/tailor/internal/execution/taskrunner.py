# -*- coding: utf-8 -*-

import importlib
import os
import shutil
import sys
from pathlib import Path
from typing import Optional

import yaql

from tailor.api.definition import DAGTask
from tailor.api.definition.core import _object_from_dict
from tailor.internal.domain.single_task import SingleTaskModel
from tailor.internal.domain.single_task import SingleTaskState
from tailor.internal.domain.workflow import WorkflowModel
from tailor.internal.service import StorageService
from tailor.internal.service import SingleTaskService, RunService, ContextService, \
    WorkflowService
from tailor.internal.utils import create_rundir, extract_real_filenames
from tailor.internal.utils import get_logger
from tailor.internal.utils import list_files


def _get_expression(arg):
    if isinstance(arg, str):
        if arg.startswith('<%') and arg.endswith('%>'):
            return arg[2:-2].strip()
        # elif arg.startswith('$'):
        #     return arg
        else:
            return ''  # why not False here?
    else:
        return False


class TaskRunner:

    def __init__(self,
                 project_name: str = None,
                 task: Optional[SingleTaskModel] = None):
        self.task_service: SingleTaskService = SingleTaskService(project_name)
        self.run_service: RunService = RunService(project_name)
        self.context_service: ContextService = ContextService(project_name)
        self.workflow_service: WorkflowService = WorkflowService(project_name)
        self.storage_service: StorageService = StorageService(project_name)

        if task:
            self.task = task  # task should be checked out with status RESERVED
            # as retrieved by db.checkout_task()
        else:
            self.task = self.task_service.checkout_ready_task()
        self.wf = self.workflow_service.find_by_id(self.task.wf_id)

        self.context = self.context_service.find_by_id(self.wf.context_id)

        # get logger
        self.logger = get_logger('TaskRunner')

        # create a run directory (here or in self.run?)
        # self.rundir = create_rundir(friendly_name=self.task.task_def["name"],
        #                             logger=self.logger)
        self.rundir = create_rundir(logger=self.logger)

        self.engine = yaql.factory.YaqlFactory().create()

    def run(self):
        self.logger.info(f'Starting task {self.task.id}')

        # step into run dir
        curdir = Path.cwd()
        os.chdir(self.rundir)

        # create a new run
        run = self.run_service.create(self.task.id, self.rundir)
        run_id = run.id

        # update task's runs
        self.task.run_ids.append(run_id)

        # update state
        self.task.state = SingleTaskState.RUNNING

        self.task_service.replace(self.task)

        # refresh workflow
        self.wf = self.workflow_service.refresh(task_id=self.task.id)

        # run task in try/except:
        try:
            self.__execute_task()
        except Exception as e:
            state = SingleTaskState.FAILED
            self.workflow_service.mark_run_failed(run_id, e)
            self.logger.error(f'Task {self.task.id} FAILED', exc_info=True)
        else:
            state = SingleTaskState.COMPLETED
            self.workflow_service.mark_run_completed(run_id, state=state)
            self.logger.info(f'Task {self.task.id} COMPLETED successfully')

        # step out of run dir
        os.chdir(curdir)
        self.__cleanup_after_run(state)
        return True

    def __execute_task(self):
        # call task-type specific code
        task_def = self.task.task_def
        if task_def['type'] == 'python':  # TODO extract constants to enum
            self.__run_python(task_def)
        elif task_def['type'] == 'duplicate':
            self.__run_duplicate(task_def)

    def __run_python(self, task_def):
        parsed_args = self.__determine_args(task_def)
        parsed_kwargs = self.__determine_kwargs(task_def)
        self.__maybe_download_files(task_def)

        action_output = self.__run_action(task_def, parsed_args, parsed_kwargs)

        self.__store_output(task_def, action_output)

        self.__maybe_upload_files(task_def, action_output)

        # update context
        self.context_service.refresh(self.context)

        # update self.wf
        # self.wf = self.workflow_service.find_by_id(self.wf.id)

    def __maybe_upload_files(self, task_def, action_output):
        upload = task_def.get('upload')
        # upload must be dict of (tag: val), where val can be:
        #   1:  one or more query expressions(str og list of str) which is applied
        #       to action_output. The query result is searched for file names
        #       pointing to existing files, these files are then uploaded to storage
        #       under the given tag.
        #   2:  one or more glob-style strings (str og list of str) which is applied
        #       in the task working dir. matching files are uploaded under the
        #       given tag.
        if upload:
            for tag, v in upload.items():
                if isinstance(v, str):
                    v = [v]
                fnames = []
                for vi in v:
                    if _get_expression(vi):  # alt 1
                        fnamesi = self.__eval_query(_get_expression(vi), action_output)
                        fnamesi = extract_real_filenames(fnamesi) or []
                    else:  # alt 2
                        fnamesi = [str(p) for p in list_files(pattern=vi)]
                    fnames.extend(fnamesi)
                if len(fnames) == 1:
                    fnames = fnames[0]
                self.storage_service.upload(self.context.storage_key,
                                            fnames,
                                            tag=tag,
                                            scope_indices=self.task.scope_indices)

    def __store_output(self, task_def, action_output):
        # TODO: walk action_output and pickle non-JSON objects.
        #       Need a mechanism to persist non-JSON objects on
        #       the storage resource
        output_to = task_def.get('output_to')
        if output_to:
            # The entire action_output is put on $.outputs.<output>
            self.context_service.add_output(self.context,
                                            output_to, action_output,
                                            self.task.scope_indices)

        output_extraction = task_def.get('output_extraction')
        if output_extraction:
            # For each (tag: query), the query is applied to action_output
            # and the result is put on $.outputs.<tag>
            for k, v in output_extraction.items():
                if _get_expression(v):
                    val = self.__eval_query(_get_expression(v), action_output)
                else:
                    raise ValueError('A sensible error message...')
                self.context_service.add_output(
                    self.context, k, val, self.task.scope_indices)

    def __run_action(self, task_def, args, kwargs):
        # do this so that python modules that have been downloaded are discovered:
        sys.path.append('.')
        # NOTE: this is potentially risky as users can execute arbitrary code by
        #       uploading python modules and calling functions in these modules
        # TODO: use 'sandbox' environment for user provided code?
        #       need to have restrictions on what can run and the python
        #       environment for which functions are executed!
        #       Look into RestrictedPython: https://github.com/zopefoundation/RestrictedPython
        # run callable
        action_name = task_def['action']
        action = self.__resolve_callable(action_name)
        self.logger.info(f'Running action: {action_name}')
        action_output = action(*args, **kwargs)
        return action_output

    def __determine_kwargs(self, task_def):
        # allow both direct kwargs from Duplicate
        # kwargs passed directly
        kwargs = task_def.get('kwargs', {})
        parsed_kwargs = self.__handle_kwargs(kwargs)
        return parsed_kwargs

    def __determine_args(self, task_def):
        args = task_def.get('args', [])
        parsed_args = self.__handle_args(args)
        return parsed_args

    def __maybe_download_files(self, task_def):
        download = task_def.get('download', [])
        file_tags = []
        download = [download] if isinstance(download, str) else download
        available_tags = list(
            self.storage_service.get_tags(self.context.storage_key).keys())
        available_fnames = self.storage_service.get_file_list(self.context.storage_key)
        for tag in download:
            if tag in available_tags or tag in available_fnames:
                file_tags.append(tag)  # download will handle tag
            else:
                raise ValueError(f'File not found: "{tag}"')
        # TODO: mechanism for local piping of files (e.g. send/receive)
        if file_tags:
            for file_tag in file_tags:
                self.storage_service.download(self.context.storage_key,
                                              file_tag,
                                              scope_indices=self.task.scope_indices)

        # TODO: download files from storage with download_filter
        # TODO: download from aux storage

    def __run_duplicate(self, task_def):
        parsed_args = self.__determine_args_duplicate(task_def)
        parsed_kwargs = self.__determine_kwargs_duplicate(task_def)
        downloaded_files = self.__maybe_download_files_duplicate(task_def)

        duplication_count = self.__determine_duplication_count(downloaded_files,
                                                               parsed_args,
                                                               parsed_kwargs, task_def)

        # we are now looking at <duplication_count> duplicates

        ### duplication ###

        # get task/workflow to duplicate
        dup_def = task_def['task_def']

        # create a dag object (to be duplicated)
        if dup_def['type'] == 'dag':
            dup_dag = DAGTask.from_dict(dup_def)
        else:  # single task, wrap it in a dag
            task = _object_from_dict(dup_def)
            dup_dag = DAGTask(task)

        self.__create_duplicated_tasks(dup_dag, duplication_count, downloaded_files,
                                       parsed_args, parsed_kwargs,
                                       task_def)

        # update self.wf
        self.wf = self.workflow_service.find_by_id(self.wf.id)

    def __create_duplicated_tasks(self, dup_dag, duplication_count, files, parsed_args,
                                  parsed_kwargs, task_def):
        new_children = []
        links_update = {}
        task_states = {}
        download = task_def.get('download')
        for i in range(duplication_count):

            # get tasks objects and links
            task_defs, p_c_links, c_p_links = dup_dag.get_task_defs_and_links()

            scope_indices = self.task.scope_indices + [i]

            # create Tasks and id-based links
            tasks, p_c_links = self.task_service.create_tasks_and_links(
                self.wf.id,
                task_defs,
                p_c_links,
                self.wf.worker,
                scope_indices
            )

            task_states_i = {t.id: t.state for t in tasks}

            # create a temporary Workflow
            dup_wf = WorkflowModel(0, 'temp', dup_dag.to_dict(),
                                   p_c_links, self.context.id, task_states_i,
                                   self.wf.external_references)

            # update task defs for root tasks
            root_ids = dup_wf.root_task_ids
            root_tasks = [task for task in tasks if task.id in root_ids]
            for root_task in root_tasks:
                root_task_def = root_task.task_def

                task_download = root_task_def.get('download')
                if task_download and download:
                    if isinstance(task_download, list):
                        task_download.extend(files[i])
                    else:  # assume single file tag
                        root_task_def['download'] = [task_download, *files[i]]
                elif download:
                    root_task_def['download'] = files[i]

                # old behaviour: append to existing args
                # task_args = root_task_def.get('args')
                # if task_args and parsed_args:
                #     task_args = parsed_args
                #
                #     if isinstance(parsed_args[i], list):
                #         task_args.extend(parsed_args[i])
                #     else:
                #         task_args.append(parsed_args[i])
                # elif parsed_args:
                #     root_task_def['args'] = parsed_args[i]

                # new behaviour: overwrite 'args' already present in the task
                if parsed_args:
                    root_task_def['args'] = parsed_args[i]

                task_kwargs = root_task_def.get('kwargs')
                if task_kwargs and parsed_kwargs:
                    # need to evaluate if kwargs is query expressions
                    if _get_expression(task_kwargs):
                        task_kwargs = self.context_service.query(
                            self.context, _get_expression(task_kwargs))
                    if _get_expression(parsed_kwargs[i]):
                        kwargs_i = self.context_service.query(
                            self.context, _get_expression(parsed_kwargs[i]))
                    else:
                        kwargs_i = parsed_kwargs[i]
                    task_kwargs.update(kwargs_i)
                elif parsed_kwargs:
                    root_task_def['kwargs'] = parsed_kwargs[i]

            # update links in dup_wf (leaf tasks)
            for task_id in dup_wf.leaf_task_ids:
                dup_wf.links[task_id].extend(self.wf.links[self.task.id])

            new_children.extend(dup_wf.root_task_ids)
            links_update.update(dup_wf.links)
            task_states.update(dup_wf.task_states)
            # insert tasks
            self.task_service.insert_many(tasks)
        # finally need to update self.wf with new links and task_states
        links_update[self.task.id] = new_children
        self.workflow_service.update_links(self.wf.id, links_update)
        self.workflow_service.update_task_states(self.wf.id, task_states)

    def __determine_duplication_count(self, downloaded_files, parsed_args,
                                      parsed_kwargs, task_def):
        ### checking ###
        # files, args and kwargs must be None/empty or same length,
        # At least one of these must be not None/empty
        if downloaded_files or parsed_args or parsed_kwargs:
            n = None
            if downloaded_files:
                n = len(downloaded_files)
            if parsed_args:
                if n:
                    if not n == len(parsed_args):
                        raise ValueError('Cannot duplicate, mismatch in files, '
                                         'args and kwargs...')
                else:
                    n = len(parsed_args)
            if parsed_kwargs:
                if n:
                    if not n == len(parsed_kwargs):
                        raise ValueError('Cannot duplicate, mismatch in files, '
                                         'args and kwargs...')
                else:
                    n = len(parsed_kwargs)
                # assert that kwargs is list of dicts
        else:
            taskname = task_def['name']
            raise ValueError('Cannot duplicate, please specify "download",'
                             ' "args", or "kwargs" in Duplicate task with name'
                             f' "{taskname}".')
        return n

    def __determine_kwargs_duplicate(self, task_def):
        kwargs = task_def.get('kwargs')
        if isinstance(kwargs, str) and _get_expression(kwargs):
            kwargs = self.context_service.query(self.context, _get_expression(kwargs))
        # make sure kwargs is list of dicts, or list of expressions
        if kwargs:
            kwargs = kwargs if isinstance(kwargs, list) else [kwargs]
            for i in range(len(kwargs)):
                if not isinstance(kwargs[i], dict) and not _get_expression(kwargs[i]):
                    raise TypeError('invalid kwarg format in duplicate')
        return kwargs

    def __determine_args_duplicate(self, task_def):
        # if these are single query strings, they need to be evaluated here
        args = task_def.get('args')
        if isinstance(args, str) and _get_expression(args):
            args = self.context_service.query(self.context, _get_expression(args))
        # make sure args is list of lists, or list of expressions
        if args:
            args = args if isinstance(args, list) else [args]
            for i in range(len(args)):
                if not isinstance(args[i], list) and not _get_expression(args[i]):
                    args[i] = [args[i]]
        return args

    def __maybe_download_files_duplicate(self, task_def):
        files = []
        # files to download
        download = task_def.get('download')
        if download:
            if isinstance(download, str):  # assume download is single file tag
                parsed_files = self.storage_service.get_scoped_file_list(
                    self.context.storage_key, download, self.task.scope_indices)
                files = [[pf] for pf in parsed_files]

            else:  # assume download is list of file tags

                # Different logic can be applied here:
                #   1. tag represents single file
                #   2. tag represents list of filenames
                #       here we have choices:
                #           - duplicate for each list
                #           - duplicate for each element in the zipped lists
                #
                #   In the general case tags in download are a combination of
                #   1 and 2 above.
                #   The following logic is applied:
                #       - tags are only single files (case 1):
                #           one duplicate for each tag
                #       - tags are both lists of filenames (case 2) and single
                #         filenames (case 1):
                #           use zip-logic for case 2, i.e. one duplicate per
                #           element in the lists. case 1 tags are downloaded
                #           to every duplicate. Duplicate no. i then gets
                #           files: [case2_tag1[i], case2_tag2[i], case1_tag].

                single_files = []
                files_to_zip = []
                for tag in download:
                    filenames = self.storage_service.get_scoped_file_list(
                        self.context.storage_key, tag, self.task.scope_indices)
                    if isinstance(filenames, str):
                        single_files.append(filenames)
                    else:
                        files_to_zip.append(filenames)
                if files_to_zip:
                    files = [list(tup) for tup in zip(*files_to_zip)]
                    for lst in files:
                        lst.extend(single_files)
                else:
                    files = [[sf] for sf in single_files]

            # files is now a list of lists of filename(s)
            # a duplicate shall be made for each element in files
        return files

    def __handle_args(self, args):
        if isinstance(args, str) and _get_expression(args):
            parsed_args = self.context_service.query(self.context,
                                                     _get_expression(args),
                                                     self.task.scope_indices)
            if not isinstance(parsed_args, list):
                parsed_args = [parsed_args]
            return parsed_args
        elif not isinstance(args, list):
            args = [args]
        parsed_args = []
        for arg in args:
            if _get_expression(arg):
                parsed_arg = self.context_service.query(self.context,
                                                        _get_expression(arg),
                                                        self.task.scope_indices)
                parsed_args.append(parsed_arg)
            else:
                parsed_args.append(arg)
        return parsed_args

    def __handle_kwargs(self, kwargs):
        if isinstance(kwargs, str) and _get_expression(kwargs):
            parsed_kwargs = self.context_service.query(self.context,
                                                       _get_expression(kwargs),
                                                       self.task.scope_indices)
        else:
            parsed_kwargs = {}
            for kw, arg in kwargs.items():
                if _get_expression(arg):
                    parsed_arg = self.context_service.query(self.context,
                                                            _get_expression(arg),
                                                            self.task.scope_indices)
                    parsed_kwargs[kw] = parsed_arg
                else:
                    parsed_kwargs[kw] = arg
        return parsed_kwargs

    def __resolve_callable(self, action_name):
        parts = action_name.split('.')
        funcname = parts[-1]
        modulename = '.'.join(parts[:-1])
        action = getattr(importlib.import_module(modulename), funcname)
        return action

    def __eval_query(self, expression, data):
        return self.engine(expression).evaluate(data=data)

    def __cleanup_after_run(self, state):
        # delete rundir
        if state == SingleTaskState.COMPLETED:  # only remove non-empty run dir if COMPLETED
            try:
                shutil.rmtree(self.rundir, ignore_errors=True)
                self.logger.info('Deleted run dir')
            except:
                self.logger.info('Could not delete run dir', exc_info=1)

            # TODO: trigger additional actions:
            #   - is this the last task of a COMPLETED workflow:
            #       - if so: cleanup temporary storage, etc.
            #   - is this the last task of a COMPLETED context group:
            #       - if so: do context mapping (update inputs of downstream contexts)

        else:  # always remove empty dirs
            try:
                self.rundir.rmdir()
                self.logger.info('Deleted empty run dir')
            except:
                # self.logger.info('Could not delete run dir', exc_info=1)
                pass


def run_task(project_name, task):
    runner = TaskRunner(project_name, task)
    return runner.run()
